By Ben Ricket, Nathaniel Banse, Alyssa Aranda, Lukas Littlejohn, and Julian Shah



We have built an autonomous robotic chair and trashcan, both of which are eager to follow people (or each other) and patiently wait behind them. 
This design solves the age-old problem of people being too lazy to walk to a chair, wishing one were immediately behind them instead. 
This target group of people too lazy to walk to a chair provides another probelm we have to address: Litter.
Given the strong correlation between people too lazy to walk to a chair, and people too lazy to throw away their trash, we had to provided a convenient receptacle for trash that would also follow the target individual.

On a technical level, our project consists of two robots, a chair and a trash can. Both of these robots have the same control scheme (differential drive with two bi-directional driven wheels and casters on the back). Additionally, both of these robots have a Raspberry Pi onboard to control them as well as an Intel Realsense depth camera for observing the environment. 
To detect and follow people, we implement two methods --- Aruco marker detection via OpenCV, which is computationally cheaper than trying to detect a human, as well as human detection via MediaPipe. Both of these methods return the coordinates in 3D space of the target relative to the robot, and the robot can drive accordingly. 
Additionally, the robots are able to communicate --- while this is not necessarily essential for simple following behavior, the robots are able to communicate the positions of targets they see to each other, potentially opening the door to more complicated coordination schemes in the future. 

We help control ther robots via a script that runs locally on our computer, providing a heartbeat to ensure the robots only drive when we have the capacity to stop them. At any moment, we can choose to override their movement and supply our own velocity commands to either robot.
The robotic chair is also a fully functional chair, capable of supporting the full weight of a person sitting on the chair while being reasonably comfortable and ergonomic.

![Finalized chair](images/duckling_follow.jpg)

<div style={{ position: "relative", paddingTop: "56.25%" }}>
  <iframe
    src="https://www.youtube.com/embed/c71_gnccJls"
    style={{
      position: "absolute",
      top: 0,
      left: 0,
      width: "100%",
      height: "100%",
    }}
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  />
</div>

### System diagram

![System diagram image](images/system_diagram.png)

### Goals

In our design process, we initially outlined a set of design goals, designed chiefly around the mechanical, electrical, and software components of the project. 

Our initial goals, from the first review, were as follows:

Mechanical:
- Weld an aluminum frame for a chair.
- Upholster chair for comfort and fit wide range of people.
- Durable drivetrain that can withstand the weight of a range of different people sitting.
- Make drivetrain back drivable.

Electrical:
- Easily replaceable/serviceable components
- No/Minimal visible wires
- Integrated e-stops
- Robot movements can be overridden by a human pushing it

Software:
- Localization relative to the followed person
- Communication of sensor data between two robots
- Image processing to identify targets to follow
- Teleop control modes and software overrides

We also outlined a preliminary software architecture sketch for our project, appearing as follows:

![early software diagram](images/early_software_diagram.png)

By the end of the second review, we had not added or removed goals from our intial list, and had made progress towards our initial goals. 

We never had to entirely re-scope our goals or change any drastically, and are currently at the following stage:

Mechanical:

- The aluminum frame is welded, supports weight, and works very well.
- The chair has not been upholstered --- however, we did study which chairs had more pleasant/ergonomic designs, and designed the chair to have a pleasant recline and soft surfaces made of wood, which are (surprisingly) quite comfortable to sit in.
- The drivetrain withstands all weight we've put on it --- it does not drive well with the weight of a person, but this was not within the design goal --- the robot simply needs to approach a person and let them sit stationarily.
- The drivetrain is back drivable, and the chair can be pushed in any way when not spinning the motors.

Electrical:

- The components are easily replaceable, with the electronic box of the chair swappable after removing four screws. The batteries are interchangeable as well.
- Most wires are contained in the body of the chair and trash can, out of view, with connectors attaching sets of wires to the right pins. Future progress on this front would involve soldering the wires to the motor leads and routing the motor control wires through the aluminum frame.
- The e-stop is mechanically integrated with the frame, yet is not currently wired up. In the future, we would want to spec a relay for the e-stop to control, in order to not risk running more than the rated 10A current through it. 
- The robot movement can be overridden by a human pushing it.

Software:

- The robots successfully localize themselves relative to a followed person and drive to approach the person, both with Aruco markers or with human detection via MediaPipe.
- The two robots communicate their camera data between each other via the shared `/pose_updates` topic.
- We successfully implemented image processing identifying Aruco markers and humans via OpenCV and MediaPipe.
- The robots can be independently controlled via keyboard input on a teleop controller script, stop upon a heartbeat from that script dropping out, and can be stopped almost immediately upon pressing SPACE. 


### Decisions + Tradeoffs

Over the course of the project, we have laid out reasoning behind the design decisions we came to making over the course of the project.

#### Having multiple robots

While this theoretically adds extra cost and requires us deal with communication between robots, implementing this inter-robot communication was among our learning goals for the project, and we were able to find most of the expensive components for free.
Additionally, the simpler trash can design allowed for us to test our initial code as soon as possible, given that the metal chair frame was only drivable late in the project. Were we not able to test code on the trash can, integration would have been even more difficult. 

#### Using ROS2

The downsides of using ROS2 are the fact that it is a fairly heavy middleware to use, is nontrivial to set up right, and is picky about Ubuntu versions. However, these cons were mitigated by the following:

ROS2 makes interprocess and inter-robot communication much easier than networking manually.

ROS2 is something both of us on software (Ben and Julian) are familiar with using.

ROS2 provides a very convenient library for handling frame transforms, tf2.

ROS2 Humble provides a Docker image that lets us use the same environment on our different Raspberry Pis.

For these reasons, we decided to go with ROS2. 


#### Not integrating encoders

In the final chair, encoders are not currently present. While we purchased encoders and spent multiple hours configuring them, reading the data sheet, and trying to interface with them, the difficulty they offered was not worth the time commitment near the close of this project. In particular, we ran into the following issues:

- Our encoders have the same I2C address without a multiplexer, so we would need to purchase an I2C multiplexer in order to read encoder data using I2C. The alternative is using PWM.
- RPi 5 does not supprot kernel level interrutps via the PiGPIO library, which makes reading PWM signals less accurate.
- Configuring the encoders to output PWM is difficult, requiring one resister be desoldered from the board, a script flashed to the Arduino to configure it, and a script carefully used to burn this configuration to write-once memory that cannot be changed in order for the configuration to persist. 
- With the non-burned encoder, we were still encountering difficulty consistently reading from the encoders.
- We had not yet designed a mount for the encoders on the chair, only for the trash can. 

#### Not integrating LiDAR

In the final chair, the LiDAR sensor is mounted, but is not run. While interfacing with the LiDAR is not difficult --- we did this by the second review date, visualizing the LiDAR scan from the sensor --- we did not end up with software logic that accounted for obstacles. We have no obstacle avoidance built into the chair and trash can, so the LiDAR data did not provide any useful input given the state of our code.

#### Overall

Overall, we're content with the progress we've made so far on the chair. While there are avenues to explore with respect to integrating the sensors we weren't able to integrate and adding support for more insteresting and complicated control modes, we are proud of having made two functional robots, one of which doubles as a chair, which are able to follow an Aruco marker or human fairly well. 

<div style={{ position: "relative", paddingTop: "56.25%" }}>
  <iframe
    src="https://www.youtube.com/embed/Kuj1wVJ1OWI"
    style={{
      position: "absolute",
      top: 0,
      left: 0,
      width: "100%",
      height: "100%",
    }}
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  />
</div>

Links to videos in case display doesn't work:
[Video 1](https://youtu.be/c71_gnccJls)
[Video 2](https://youtu.be/Kuj1wVJ1OWI)